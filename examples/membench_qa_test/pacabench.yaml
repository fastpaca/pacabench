name: membench-qa-test
description: Recreation of QA test for long-context & mem0
version: "1.0.0"

config:
  concurrency: 10
  timeout_seconds: 600
  proxy:
    enabled: true
    provider: "openai"

agents:
  - name: "long-context-baseline"
    command: "python agents/long_context_agent.py"
    env:
      OPENAI_API_KEY: "${OPENAI_API_KEY}"

  - name: "mem0"
    command: "python agents/mem0_agent.py"
    env:
      OPENAI_API_KEY: "${OPENAI_API_KEY}"

  - name: "zep-graphiti"
    command: "python agents/zep_graphiti_agent.py"
    env:
      OPENAI_API_KEY: "${OPENAI_API_KEY}"

datasets:
  - name: "membench"
    source: "git:https://github.com/import-myself/Membench.git"
    prepare: "python scripts/prepare_membench.py"
    input_map:
      input: "question"
      expected: "ground_truth"
    evaluator:
      type: "multiple_choice"
      # Fallback to F1 when choices are missing; avoids LLM judge by default.
      extra_config:
        fallback: "f1"

output:
  directory: "./runs"
